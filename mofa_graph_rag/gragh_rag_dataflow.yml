# dataflow.yml
# Defines the data flow for the LightRAG processing pipeline using MoFA/DORA.
# Starts with interactive terminal input.

nodes:
  # --------------------------------------------------------------------------
  # Node 1: Terminal Input (Initiates the pipeline)
  # --------------------------------------------------------------------------
  # Waits for user input in the terminal to start the pipeline.
  - id: terminal-input
    # Configuration provided by the user.
    # IMPORTANT: Ensure the 'build' path is correct relative to where you run 'dora start'.
    #            The path '../../node-hub/terminal-input' might need adjustment.
    #            Also, verify if 'path: dynamic' works in your setup or if a specific path is needed.
    build: pip install -e ../../node-hub/terminal-input # <-- VERIFY THIS PATH
    path: dynamic                                       # <-- VERIFY THIS PATH/MECHANISM
    outputs:
      # Outputs the user's typed input (or just a signal if Enter is pressed)
      - data
    # No inputs are needed for this node to simply *start* the pipeline.
    # The inputs shown in the original user example were likely for displaying
    # results back to the terminal *after* processing, not for initiation.
    inputs: {}

  # --------------------------------------------------------------------------
  # Node 2: Data Preparation (Step 0 Operator)
  # --------------------------------------------------------------------------
  # Downloads the dataset and extracts unique contexts. Triggered by terminal input.
  - id: data-prep
    operator:
      python: scripts/data_prep_operator.py
    # Inputs: Now waits for the 'data' output from the terminal-input node.
    # The key 'trigger' is arbitrary here; the operator code just checks for any input event.
    inputs:
      trigger: terminal-input/data
    outputs:
      # The directory path where unique context JSON files are saved.
      - unique_contexts_dir

  # --------------------------------------------------------------------------
  # Node 3: RAG Indexing (Step 1 Operator)
  # --------------------------------------------------------------------------
  # Builds the LightRAG index using the unique contexts. Depends on data-prep.
  - id: rag-indexer
    operator:
      python: scripts/rag_index_operator.py
    # Inputs: Depends on the output of the data preparation step.
    # The key 'unique_contexts_dir' matches the expected event_id in the operator.
    inputs:
      unique_contexts_dir: data-prep/unique_contexts_dir
    outputs:
      # The path to the directory containing the initialized RAG index.
      - rag_index_dir

  # --------------------------------------------------------------------------
  # Node 4: Question Generation (Step 2 Operator)
  # --------------------------------------------------------------------------
  # Generates questions based on the unique contexts. Depends on data-prep.
  # Runs in parallel (logically) with rag-indexer.
  - id: question-generator
    operator:
      python: scripts/question_gen_operator.py
    # Inputs: Also depends on the output of the data preparation step.
    # The key 'unique_contexts_dir' matches the expected event_id in the operator.
    inputs:
      unique_contexts_dir: data-prep/unique_contexts_dir
    outputs:
      # The path to the text file containing generated questions.
      - generated_questions_file

  # --------------------------------------------------------------------------
  # Node 5: RAG Querying (Step 3 Operator)
  # --------------------------------------------------------------------------
  # Queries the RAG index using the generated questions. Depends on indexer and generator.
  - id: rag-querier
    operator:
      python: scripts/rag_query_operator.py
    # Inputs: Depends on BOTH the index being ready AND the questions file.
    # DORA ensures this node runs only after receiving inputs on *both* keys.
    inputs:
      # Key 'rag_index_dir' matches expected event_id in the operator.
      rag_index_dir: rag-indexer/rag_index_dir
      # Key 'generated_questions_file' matches expected event_id in the operator.
      generated_questions_file: question-generator/generated_questions_file
    outputs:
      # A payload containing paths to the results and errors JSON files.
      - query_results

  # --------------------------------------------------------------------------
  # Node 6: Result Logger (Sink Node)
  # --------------------------------------------------------------------------
  # Receives and logs the final output of the pipeline. Uses external script.
  - id: result-logger
    operator:
      python: scripts/result_logger_operator.py
    # Inputs: Receives the final payload from the RAG querying step.
    # The key 'final_output' is arbitrary for this simple logger.
    inputs:
      final_output: rag-querier/query_results
    # No outputs needed for this sink node.